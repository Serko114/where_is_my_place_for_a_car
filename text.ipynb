{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d3f992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12 + 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc7f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2132, 0.0944, 0.4950],\n",
      "        [0.3753, 0.5556, 0.2424],\n",
      "        [0.3507, 0.0588, 0.9137],\n",
      "        [0.0999, 0.9816, 0.5770],\n",
      "        [0.9749, 0.3135, 0.9517]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250c6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# блок переименовывания картинок:\n",
    "for _,i in enumerate(os.listdir('data/фото_для_детекции/фото')):\n",
    "    photo = f'data/фото_для_детекции/фото/{i}'\n",
    "    new_photo = f'data/фото_для_детекции/фото/looking_for_a_fox_{_}.png'\n",
    "    os.rename(photo, new_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea592b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1080, 1920)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# меняем каналы массива np.array:\n",
    "pic = cv2.imread('130625_1_001.png').transpose(-1,0,1)\n",
    "pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c95999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# блок просмотра картинки:\n",
    "cv2.imshow('image',pic[2])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08ab4e",
   "metadata": {},
   "source": [
    "### ***Собираем сегментацию***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import random \n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import albumentations as albu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    \"background\",\n",
    "    \"ground\", \n",
    "    \"no_car\", \n",
    "]\n",
    "DATSET_NAME = \"data/1_sum_data_segmentation\"\n",
    "LABEL_COLORS_FILE = f\"{DATSET_NAME}/label_colors.txt\"\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import numpy as np\n",
    "class Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        masks_dir,\n",
    "        augmentation=None,\n",
    "        preprocessing=None\n",
    "    ):\n",
    "        self.images_paths = glob(f\"{images_dir}/*\")\n",
    "        self.masks_paths = glob(f\"{masks_dir}/*\")\n",
    "\n",
    "        self.cls_colors = self._get_classes_colors(LABEL_COLORS_FILE)\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def _get_classes_colors(self, label_colors_dir):\n",
    "        cls_colors = {}\n",
    "        with open(label_colors_dir) as file:\n",
    "            while line := file.readline():\n",
    "                R, G, B, label = line.rstrip().split()\n",
    "                cls_colors[label] = np.array([B, G, R], dtype=np.uint8)\n",
    "\n",
    "        keyorder = CLASSES\n",
    "        cls_colors_ordered = {}\n",
    "        for k in keyorder:\n",
    "            if k in cls_colors:\n",
    "                cls_colors_ordered[k] = cls_colors[k]\n",
    "            elif k==\"background\":\n",
    "                cls_colors_ordered[k] = np.array([0, 0, 0], dtype=np.uint8)\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected label {k}, cls colors: {cls_colors}\")\n",
    "\n",
    "        return cls_colors_ordered\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.imread(self.images_paths[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(self.masks_paths[i])\n",
    "        masks = [cv2.inRange(mask, color, color) for color in self.cls_colors.values()]\n",
    "        masks = [(m > 0).astype(\"float32\") for m in masks]\n",
    "        mask = np.stack(masks, axis=-1).astype(\"float\")\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_imshow = {\n",
    "        \"background\" : np.array([0, 0, 0]),\n",
    "        \"ground\" : np.array([204, 153, 51]),\n",
    "        \"no_car\" : np.array([255, 96, 55]),\n",
    "    }\n",
    "\n",
    "\n",
    "def _colorize_mask(mask: np.ndarray):\n",
    "    mask = mask.squeeze()\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    square_ratios = {}\n",
    "    for cls_code, cls in enumerate(CLASSES):\n",
    "        cls_mask = mask == cls_code\n",
    "        square_ratios[cls] = cls_mask.sum() / cls_mask.size\n",
    "        colored_mask += np.multiply.outer(cls_mask, colors_imshow[cls]).astype(np.uint8)\n",
    "    #-----------------------------------------------------------\n",
    "    # добавил коэффициент выход: отношение точек машин к точкам парковки:\n",
    "    coeff = np.around(square_ratios['ground'] / square_ratios['no_car'], 4)\n",
    "    print(coeff)\n",
    "    #-----------------------------------------------------------\n",
    "    return colored_mask, square_ratios, coeff\n",
    "\n",
    "\n",
    "def reverse_normalize(img, mean, std):\n",
    "    # Invert normalization\n",
    "    img = img * np.array(std) + np.array(mean)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_predicts(img: np.ndarray, mask_gt: np.ndarray, mask_pred: np.ndarray, normalized=False):\n",
    "    # размер img: H, W, CHANNEL\n",
    "    # размер mask_gt, mask_pred: H, W, значения - range(len(CLASSES)\n",
    "    # coeff = []\n",
    "    _, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    # print([square_ratios[cls] for cls in CLASSES])\n",
    "    if normalized:\n",
    "        # Reverse the normalization to get the unnormalized image\n",
    "        img = reverse_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    axes[0].imshow(img)\n",
    "    \n",
    "    # axes[0].set_title('ngurnguir')\n",
    "    \n",
    "    mask_gt, square_ratios, coeff_real = _colorize_mask(mask_gt)\n",
    "    title = \"Площади:\\n\" + f'COEFF_real: {str(coeff_real)}\\n' + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[1].imshow(mask_gt, cmap=\"twilight\")\n",
    "    axes[1].set_title(f\"GT маска\\n\" + title)\n",
    "\n",
    "    mask_pred, square_ratios, coeff_pre = _colorize_mask(mask_pred)\n",
    "    title = \"Площади:\\n\" + f'COEFF_pred: {str(coeff_pre)}\\n' + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[2].imshow(mask_pred, cmap=\"twilight\")\n",
    "    axes[2].set_title(f\"PRED маска\\n\" + title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return coeff_real, coeff_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentation():\n",
    "    test_transform = [albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),\n",
    "    albu.PadIfNeeded(min_height=INFER_HEIGHT, min_width=INFER_WIDTH, border_mode=2, always_apply=True),\n",
    "    albu.CenterCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True)]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    # Осуществит стартовую нормализацию данных согласно своим значениям или готовым для imagenet\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATSET_NAME = \"data/1_sum_data_segmentation\"\n",
    "\n",
    "X_TRAIN_DIR = f\"{DATSET_NAME}/Train\"\n",
    "Y_TRAIN_DIR = f\"{DATSET_NAME}/Trainannot\"\n",
    "\n",
    "X_VALID_DIR = f\"{DATSET_NAME}/Validation\"\n",
    "Y_VALID_DIR = f\"{DATSET_NAME}/Validationannot\"\n",
    "\n",
    "X_TEST_DIR = f\"{DATSET_NAME}/Validation\"\n",
    "Y_TEST_DIR = f\"{DATSET_NAME}/Validationannot\"\n",
    "\n",
    "LABEL_COLORS_FILE = f\"{DATSET_NAME}/label_colors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    \"background\",\n",
    "    \"ground\", \n",
    "    \"no_car\", \n",
    "]\n",
    "ENCODER = 'resnet18'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'softmax2d' \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EPOCHS = 70\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "INIT_LR = 0.0005\n",
    "LR_DECREASE_STEP = 15\n",
    "LR_DECREASE_COEF = 2 # LR будет разделен на этот коэф раз в LR_DECREASE_STEP эпох\n",
    "\n",
    "INFER_WIDTH = 256\n",
    "INFER_HEIGHT = 256\n",
    "\n",
    "# loss = utils.losses.DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58078d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.jit.load('models/best_segmentation.pt', map_location=DEVICE)\n",
    "\n",
    "x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "pr_mask = best_model(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93261c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка IoU\n",
    "test_dataset = Dataset(\n",
    "    X_TEST_DIR, \n",
    "    Y_TEST_DIR, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "coff_real = []\n",
    "coff_pre = []\n",
    "coff_diffr = []\n",
    "for i in range(len(test_dataset)):\n",
    "    # n = np.random.choice(len(test_dataset))\n",
    "    print(i)\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model(x_tensor)\n",
    "    pr_mask = pr_mask.squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    label_mask = np.argmax(pr_mask, axis=0)\n",
    "    print(label_mask.shape, image.shape, gt_mask.shape)\n",
    "    # print(np.argmax(gt_mask, axis=0))\n",
    "    # visualize_predicts(image, np.argmax(gt_mask, axis=0), label_mask, normalized=True)\n",
    "    real, pre = visualize_predicts(image, np.argmax(gt_mask, axis=0), label_mask, normalized=True)\n",
    "    coff_real.append(real)\n",
    "    coff_pre.append(pre)\n",
    "    coff_diffr.append(np.abs(np.around(real - pre, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fa24c",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0f23c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PROJECTS\\where_is_my_place_for_a_car\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import cv2\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d04e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from albumentations.pytorch import ToTensorV2 \n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab9af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_ch, out_ch):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(out_ch),                                                            \n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1), \n",
    "        nn.BatchNorm2d(out_ch),                                                            \n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    \n",
    "    return conv\n",
    "\n",
    "#def cropper(og_tensor, target_tensor):\n",
    "#    og_shape = og_tensor.shape[2]\n",
    "#    target_shape = target_tensor.shape[2]\n",
    "#    delta = (og_shape - target_shape) // 2\n",
    "#    cropped_og_tensor = og_tensor[:,:,delta:og_shape-delta,delta:og_shape-delta]\n",
    "#    return cropped_og_tensor\n",
    " \n",
    "    \n",
    "def padder(left_tensor, right_tensor): \n",
    "    # left_tensor is the tensor on the encoder side of UNET\n",
    "    # right_tensor is the tensor on the decoder side  of the UNET\n",
    "    \n",
    "    if left_tensor.shape != right_tensor.shape:\n",
    "        padded = torch.zeros(left_tensor.shape)\n",
    "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
    "        return padded.to(CFG.device)\n",
    "    \n",
    "    return right_tensor.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483dfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,in_chnls, n_classes):\n",
    "        super(UNET,self).__init__()\n",
    "        \n",
    "        self.in_chnls = in_chnls\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.down_conv_1 = double_conv(in_ch=self.in_chnls,out_ch=64)\n",
    "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128)\n",
    "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256)\n",
    "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512)\n",
    "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024)\n",
    "        #print(self.down_conv_1)\n",
    "        \n",
    "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
    "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
    "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
    "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
    "        \n",
    "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512)\n",
    "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256)\n",
    "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128)\n",
    "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64)\n",
    "        \n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.n_classes,kernel_size=1,stride=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # encoding\n",
    "        x1 = self.down_conv_1(x)\n",
    "        #print(\"X1\", x1.shape)\n",
    "        p1 = self.max_pool(x1)\n",
    "        #print(\"p1\", p1.shape)\n",
    "        x2 = self.down_conv_2(p1)\n",
    "        #print(\"X2\", x2.shape)\n",
    "        p2 = self.max_pool(x2)\n",
    "        #print(\"p2\", p2.shape)\n",
    "        x3 = self.down_conv_3(p2)\n",
    "        #print(\"X2\", x3.shape)\n",
    "        p3 = self.max_pool(x3)\n",
    "        #print(\"p3\", p3.shape)\n",
    "        x4 = self.down_conv_4(p3)\n",
    "        #print(\"X4\", x4.shape)\n",
    "        p4 = self.max_pool(x4)\n",
    "        #print(\"p4\", p4.shape)\n",
    "        x5 = self.down_conv_5(p4)\n",
    "        #print(\"X5\", x5.shape)\n",
    "        \n",
    "        # decoding\n",
    "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
    "        pad1 = padder(x4,d1) # padding d1 to match x4 shape\n",
    "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
    "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
    "        \n",
    "        d2 = self.up_conv_trans_2(uc1)\n",
    "        pad2 = padder(x3,d2)\n",
    "        cat2 = torch.cat([x3,pad2],dim=1)\n",
    "        uc2 = self.up_conv_2(cat2)\n",
    "        \n",
    "        d3 = self.up_conv_trans_3(uc2)\n",
    "        pad3 = padder(x2,d3)\n",
    "        cat3 = torch.cat([x2,pad3],dim=1)\n",
    "        uc3 = self.up_conv_3(cat3)\n",
    "        \n",
    "        d4 = self.up_conv_trans_4(uc3)\n",
    "        pad4 = padder(x1,d4)\n",
    "        cat4 = torch.cat([x1,pad4],dim=1)\n",
    "        uc4 = self.up_conv_4(cat4)\n",
    "        \n",
    "        conv_1x1 = self.conv_1x1(uc4)\n",
    "        return conv_1x1\n",
    "        #print(conv_1x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67fc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = UNET(in_chnls = 3, n_classes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f750e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_TRAINED = \"../input/unet-4-epoch-trained/unet_scratch.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
