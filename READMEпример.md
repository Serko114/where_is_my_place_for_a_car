# Эмоции человека

## Цель: вывесли статистику настроения человека в БД и использование pgAdmin (все в docker)
>Среду ставить на python 3.10.16, tensorflow==2.16.1, torch==1.13.1(если ставить 2.6 в блоке детекция ошибка) - ставить эту версию, чтобы ошибки не было, остальные пакеты норм.

![Пример](content_for_readme/для_заставки.png)



>Важно! Установка пакетов requirements.txt, прежде чем егозапускать с помощью **pip install -r requirements.txt**, нужно создать файлик .py и .ipynb, после чего создаем среду .conda выбираем ее для .py (не забываем добавить среду в переменные окружения windows), в файлике .ipynb создаем простую мат. операцию и запускаем, после чего появится запрос на подключение kernel, выбираем нашу среду .conda. После этих манипуляций проблем с requirements.txt не должно возникнуть.

## 1. Обучение модели.


**1.2 Разбиваем видео на кадры, получая картинки в формате png**

Программка: [FFmpeg](https://ffmpeg.org/download.html).
Чтобы она работала в командной строке, необходимо в "Переменных среды" прописать путь _C:\\ffmpeg\bin_. После этого в командной сроке пишем:
```
ffmpeg -i training_model/video/video.mp4 -vf fps=3.0 training_model/images_train/1_img%03d.png
```
>* ffmpeg -  это команда для запуска программы FFmpeg, которая используется для обработки
видео и аудио файлов
>* --i training_model/video/video.mp4 - это опция для указания входного файла train.mp4
>* -vf fps=1.5 - это опция для указания фильтрации видео с заданной частотой кадров в 1.5 кадра
в секунду
>* training_model/images_train/1_img%03d.png - это опция для указания выходной директории и формата имени 
файлов, где %03d означает использование трех цифр для нумерации файлов (папку для картинок нужно создать вручную)


**1.3 Разметка картинок с помощью [CVAT](https://www.cvat.ai/).**

 Очень удобный редактор, в моем случае я его поднял локально, используя [образ Docker](https://docs.cvat.ai/docs/administration/basics/installation/).

Т.к. размечать все картинки очень лень, я разметил около 16-ти картинок, дообучил модель 
YOLOv8m, прогнал всю пачку картинок через модель и получил неплохую разметку, после чего убрал
 лишние боксы, добавил недостающие и уточнил полезные. Далее будет описание обработки основной 
 парии картинок, но такую стратегию надо выбирать и для обучения предварительных 16-ти картинок.

 > > ! При экспорте размеченных картинок из CVAT нужно выбрать формат YOLO 1.1

**1.4 Формирование картинок с аннотациями для подачи в модель YOLO.**

Сильно много писать здесь не буду, есть 
[хороший репозиторий](https://github.com/ankhafizov/CVAT2YOLO) на GitHub,
 в котором все подробно объяснено. Единственное, что хотельсь бы добавить,
 виртуальную среду лучше ставить на python 3.10, иначе вроде там разные
 ошибки будут вылетать.

**1.5 Обучаем модель.**

У меня нет карты с CUDA, но обучать быстро очень хочется, поэтому обучение проводилось 
с помощью GPU _Google Colab_ с подгрузкой данных на _Google Диск_.

Я выложил в папку _study файлик _Обучение_YOLO_данные_с_googgledisk.ipynb_, который я использовал для обучения. Там понадобятся первых 
3 блока. Первый загружает _Google Диск_ (нужно его завести), второй подгружает модели 
ultralytics (включает и YOLOv8m), в третьем уже происходит обучение модели (нужно не забыть
выбрать GPU, а то будет обучаться до второго пришествия). Я обучал на протяжении 250-ти эпох.

![Результат](content_for_readme/results.jpg)

После обучения скачиваем из папки _/content/runs/detect/train/weights_ файл best.pt, это 
файлик с лучшими весами, т.е. весами которые выдали лучший результат по статистикам.
Т.е. в дальнейшем вместо файлика yolov8m.pt используем best.pt (можно переименовать, если
 не нравится.)

**1.6 Пропускаем видео через модель.**

За запись видео отвечает файлик VideoSaverNode.py в папке nodes, нужно не забыть в confog.yaml установить save_video : True, а когла записывать уже не надо установить False, я файлик записывал, чтобы создать gif-ку, чтобы получить видео ниже:

![Результат](content_for_readme/video.gif)

## 2. База данных.

Будем собирать статистику в реальном времени.

>Если БАЗА ДАННЫХ НЕ СОЗДАЕТСЯ:
>Переменные среды Postgres и сценарии инициализации используются только в том случае, если Postgres не находит существующую базу данных при запуске.
>![Результат](content_for_readme/errors.png)
>Удалите существующую базу данных, удалив том postgres_data, а затем запустите контейнер Postgres. Тогда он увидит пустой том и создаст для вас базу данных с использованием ваших переменных и скриптов.
Т.Е. ПРОБЛЕМА В ТОМАХ, 

**2.1 Подгружаем БД PostgereSQL и pgadmin4 все в контейнерах**
Подсказка по подключению БД к pgadmin4 указана ниже:

![Результат](content_for_readme/connection_.png)


>Еще подсказка: **docker attach [id контейнера]** - это слушать контейнер, чтобы отлавливать ошибки


## 3. Вид БД.
Вот в такой форме данные выводятся в postgereSQL:
![Результат](content_for_readme/table.png)

**id** - это номер кадра

**track_id** - номер трека, вдруг другое лицо появится

**x-left - y_up** - это координаты ограничивающих рамок

**class** - это текущая эмоция

**calm - evil** - бинарное отражение наличия эмоций в кадре есть/нет

**conf** - коэффициент уверенности в правильности предсказания


## 4. Подключение Grafana к БД.

Блок Grafana добавляем в compose-файл, затем подключаем Grafana к postgres вот так вот (параметры, которых нет на картинке - без изменений):

![Результат](content_for_readme/grafana_подключение.png)

Статистики Grafana:

![Результат](content_for_readme/emotions_Grafana_short_cut.gif)

конец